{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "91482930-31e8-4bc6-849c-4a53896499e3",
   "metadata": {},
   "source": [
    "Questão 1\n",
    "A figura apresentada mostra as regiões de decisão de quatro classificadores SVM diferentes aplicados a um problema binário, onde cada ponto representa um exemplo do conjunto de treino com duas classes distintas, representadas pelas cores vermelho e azul.\n",
    "\n",
    "1.a) Quantidade de erros em cada SVM\n",
    "Ao observar as regiões coloridas, que indicam a predição feita pelo classificador, é possível contar os pontos que estão incorretamente classificados, ou seja, pontos que pertencem a uma classe, mas estão localizados na região de decisão da classe oposta.\n",
    "\n",
    "Na SVM LinearSVC (kernel linear), observe que há um único ponto azul dentro da região vermelha, indicando um erro de classificação. Portanto, essa SVM comete 1 erro no conjunto de treino.\n",
    "\n",
    "Na SVM padrão com kernel linear, há um ponto vermelho dentro da região azul e um ponto azul dentro da região vermelha, totalizando 2 erros.\n",
    "\n",
    "Para a SVM com kernel RBF, todos os pontos estão corretamente classificados, sem nenhum exemplo fora da sua região correspondente. Logo, essa SVM apresenta 0 erros.\n",
    "\n",
    "Por fim, na SVM com kernel polinomial de grau 3, nota-se um ponto vermelho na região azul e um ponto azul na região vermelha, totalizando também 2 erros.\n",
    "\n",
    "1.b) Qual a melhor SVM?\n",
    "A melhor SVM, olhando exclusivamente para o desempenho no conjunto de treino, é a SVM com kernel RBF, pois é a única que não comete nenhum erro de classificação. Isso indica que sua região de decisão consegue capturar melhor a complexidade da distribuição dos dados, sendo capaz de lidar com separações não lineares entre as classes.\n",
    "\n",
    "Entretanto, é importante salientar que um ajuste perfeito no conjunto de treino pode levar a um modelo que generaliza mal para dados novos (overfitting). Por isso, a escolha final do modelo deve considerar também o desempenho em conjuntos de validação e teste.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3e7d02-4fac-4f7d-bf48-fb0661d6dc65",
   "metadata": {},
   "source": [
    "Questão 2 – Efeito do parâmetro de regularização C na SVM\n",
    "O parâmetro C na SVM controla a regularização do modelo, ou seja, o equilíbrio entre maximizar a margem de separação e minimizar os erros de classificação no conjunto de treino.\n",
    "Quando C é grande, o modelo penaliza fortemente os erros, buscando classificar todos os exemplos corretamente, o que tende a gerar uma margem mais estreita e menos vetores de suporte, resultando em um modelo mais simples e rígido.\n",
    "Por outro lado, quando C é pequeno, o modelo permite mais erros no conjunto de treino, favorecendo margens maiores e uma maior quantidade de vetores de suporte. Isso torna o modelo mais flexível e complexo, pois ele tolera um certo grau de violação na margem para capturar melhor padrões complexos dos dados.\n",
    "No caso apresentado, se desejamos aumentar o número de vetores de suporte para tornar o modelo mais complexo, devemos diminuir o valor do parâmetro \n",
    "C. Assim, a SVM permite mais pontos próximos ou mesmo dentro da margem, aumentando o conjunto de vetores de suporte."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e41a02-dedc-44e1-a6e7-a1cfe92daf7e",
   "metadata": {},
   "source": [
    "Questão 3 – Comparação do custo computacional entre SVM linear e perceptron\n",
    "\n",
    "Neste problema, temos um classificador SVM linear treinado com vetores de entrada de dimensão \\( K=5 \\) e um total de 450 vetores de suporte. Para classificar um novo exemplo, a SVM calcula o somatório dos produtos internos entre o vetor de entrada e cada vetor de suporte, multiplicado pelos coeficientes duais.\n",
    "\n",
    " Cálculo do custo computacional da SVM\n",
    "\n",
    "Cada produto interno entre dois vetores de dimensão 5 requer:\n",
    "\n",
    "- 5 multiplicações\n",
    "- 4 adições\n",
    "\n",
    "Logo, para um vetor de entrada, a SVM realiza 450 desses produtos internos, totalizando:\n",
    "\n",
    "\\[\n",
    "C_{\\text{original}} = 450 \\times (5 + 4) = 450 \\times 9 = 4050 \\quad \\text{operações}\n",
    "\\]\n",
    "\n",
    "## Cálculo do custo computacional do perceptron\n",
    "\n",
    "Ao converter a SVM para um perceptron, o classificador é representado por um único vetor de pesos, com o qual realiza um único produto interno:\n",
    "\n",
    "\\[\n",
    "C_{\\text{perceptron}} = 5 \\text{ multiplicações} + 4 \\text{ adições} = 9 \\quad \\text{operações}\n",
    "\\]\n",
    "\n",
    "## Fator de redução do custo\n",
    "\n",
    "O fator de redução \\( F \\) no custo computacional é dado por:\n",
    "\n",
    "\\[\n",
    "F = \\frac{C_{\\text{original}}}{C_{\\text{perceptron}}} = \\frac{4050}{9} = 450\n",
    "\\]\n",
    "\n",
    "Ou seja, ao converter a SVM para um perceptron, o custo computacional na fase de teste é reduzido em um fator de 450, o que representa uma economia significativa em termos de multiplicações e somas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b63fd0f7-9569-4d65-8bb5-b64af2ae56a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custo computacional SVM (multiplicações + adições): 4050\n",
      "Custo computacional Perceptron (multiplicações + adições): 9\n",
      "Fator de redução do custo: 450.0\n",
      "\n",
      "Saída da função de decisão:\n",
      "SVM: -42.3760\n",
      "Perceptron: -42.3760\n",
      "Diferença absoluta entre SVM e Perceptron: 0.00000000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Dimensão dos vetores de entrada\n",
    "K = 5\n",
    "\n",
    "# Número de vetores de suporte na SVM\n",
    "num_SVs = 450\n",
    "\n",
    "# Simulação dos vetores de suporte (aleatórios)\n",
    "support_vectors = np.random.randn(num_SVs, K)\n",
    "\n",
    "# Coeficientes duals (aleatórios)\n",
    "dual_coefs = np.random.randn(num_SVs)\n",
    "\n",
    "# Vetor de entrada a ser classificado\n",
    "z = np.random.randn(K)\n",
    "\n",
    "# Função para calcular custo computacional (número de operações)\n",
    "def cost_operations(num_multiplications, num_additions):\n",
    "    return num_multiplications + num_additions\n",
    "\n",
    "# Custo por produto interno entre dois vetores de dimensão K:\n",
    "# multiplicações = K\n",
    "# adições = K-1\n",
    "cost_per_dot = cost_operations(K, K-1)\n",
    "\n",
    "# Custo total SVM: número de SVs vezes custo de cada produto interno\n",
    "cost_svm = num_SVs * cost_per_dot\n",
    "\n",
    "# Custo perceptron: apenas um produto interno\n",
    "cost_perceptron = cost_per_dot\n",
    "\n",
    "print(f\"Custo computacional SVM (multiplicações + adições): {cost_svm}\")\n",
    "print(f\"Custo computacional Perceptron (multiplicações + adições): {cost_perceptron}\")\n",
    "print(f\"Fator de redução do custo: {cost_svm / cost_perceptron:.1f}\")\n",
    "\n",
    "# Classificação com SVM: soma dos produtos internos ponderados + bias\n",
    "# Simulando bias arbitrário\n",
    "bias = 0.5\n",
    "\n",
    "svm_decision = np.sum(dual_coefs * np.dot(support_vectors, z)) + bias\n",
    "\n",
    "#SVM ---> perceptron: vetor peso w\n",
    "w = np.sum(dual_coefs[:, np.newaxis] * support_vectors, axis=0)\n",
    "\n",
    "perceptron_decision = np.dot(w, z) + bias\n",
    "\n",
    "print(\"\\nSaída da função de decisão:\")\n",
    "print(f\"SVM: {svm_decision:.4f}\")\n",
    "print(f\"Perceptron: {perceptron_decision:.4f}\")\n",
    "\n",
    "# Verificando se as saídas são iguais/proximas\n",
    "print(f\"Diferença absoluta entre SVM e Perceptron: {abs(svm_decision - perceptron_decision):.8f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58faf707-a4a2-401d-a6db-36a1cb9d3052",
   "metadata": {},
   "source": [
    "## Questão 4 – Interpretação da função de decisão de uma SVM linear\n",
    "\n",
    "Temos os seguintes dados do treinamento de uma SVM linear:\n",
    "\n",
    "- Vetores de suporte:\n",
    "  \\[\n",
    "  \\mathbf{x}_1 = [1, 4], \\quad \\mathbf{x}_2 = [-2, 3], \\quad \\mathbf{x}_3 = [-2, -5]\n",
    "  \\]\n",
    "\n",
    "- Coeficientes duais (\\(\\lambda\\)):\n",
    "  \\[\n",
    "  \\lambda = [-0.5, -0.3, 0.8]\n",
    "  \\]\n",
    "\n",
    "- Bias (intercepto):\n",
    "  \\[\n",
    "  b = -2\n",
    "  \\]\n",
    "\n",
    "### a) Função de decisão geral da SVM\n",
    "\n",
    "A função de decisão é dada por:\n",
    "\\[\n",
    "f(z) = \\sum_{i=1}^3 \\lambda_i \\langle \\mathbf{x}_i, z \\rangle + b\n",
    "\\]\n",
    "Substituindo os valores:\n",
    "\\[\n",
    "f(z) = -0.5 \\langle [1,4], z \\rangle - 0.3 \\langle [-2,3], z \\rangle + 0.8 \\langle [-2,-5], z \\rangle - 2\n",
    "\\]\n",
    "\n",
    "### b) Função de decisão como perceptron\n",
    "\n",
    "Calculamos o vetor peso \\(w\\):\n",
    "\\[\n",
    "w = \\sum_{i=1}^3 \\lambda_i \\mathbf{x}_i = (-0.5)[1,4] + (-0.3)[-2,3] + 0.8[-2,-5] = [-1.5, -6.9]\n",
    "\\]\n",
    "\n",
    "Assim,\n",
    "\\[\n",
    "f(z) = \\langle w, z \\rangle + b = \\langle [-1.5, -6.9], z \\rangle - 2\n",
    "\\]\n",
    "\n",
    "### c) Saída da função de decisão para \\(z = [0,0]\\)\n",
    "\n",
    "Calculando:\n",
    "\\[\n",
    "f([0,0]) = -2\n",
    "\\]\n",
    "A predição é:\n",
    "\\[\n",
    "y = \\begin{cases}\n",
    "1, & \\text{se } f(z) > 0 \\\\\n",
    "0, & \\text{caso contrário}\n",
    "\\end{cases}\n",
    "\\]\n",
    "Portanto,\n",
    "\\[\n",
    "y = 0\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### Código Python para cálculo e predição\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Vetores de suporte\n",
    "support_vectors = np.array([\n",
    "    [1, 4],\n",
    "    [-2, 3],\n",
    "    [-2, -5]\n",
    "])\n",
    "\n",
    "# Coeficientes duais\n",
    "dual_coefs = np.array([-0.5, -0.3, 0.8])\n",
    "\n",
    "# Bias\n",
    "bias = -2\n",
    "\n",
    "# Calcula vetor peso w\n",
    "w = np.sum(dual_coefs[:, np.newaxis] * support_vectors, axis=0)\n",
    "print(f\"Vetor peso w: {w}\")\n",
    "\n",
    "# Vetor de entrada para teste\n",
    "z = np.array([0, 0])\n",
    "\n",
    "# Calcula f(z)\n",
    "f_z = np.dot(w, z) + bias\n",
    "print(f\"f(z) para z = {z}: {f_z}\")\n",
    "\n",
    "# Predição\n",
    "y_pred = 1 if f_z > 0 else 0\n",
    "print(f\"Predição para z = {z}: {y_pred}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0be3112-513e-4320-85e5-346d8313b603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vetor peso w: [-1.5 -6.9]\n",
      "f(z) para z = [0 0]: -2.0\n",
      "Predição para z = [0 0]: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Vetores de suporte (support vectors)\n",
    "support_vectors = np.array([\n",
    "    [1, 4],\n",
    "    [-2, 3],\n",
    "    [-2, -5]\n",
    "])\n",
    "\n",
    "# Coeficientes duais (dual coefficients)\n",
    "dual_coefs = np.array([-0.5, -0.3, 0.8])\n",
    "\n",
    "# Bias (intercept)\n",
    "bias = -2\n",
    "\n",
    "# Função para calcular o vetor peso w\n",
    "def calculate_w(support_vectors, dual_coefs):\n",
    "    # Combinação linear dos vetores de suporte ponderados pelos coeficientes duais\n",
    "    w = np.sum(dual_coefs[:, np.newaxis] * support_vectors, axis=0)\n",
    "    return w\n",
    "\n",
    "# Função de decisão f(z) = <w, z> + b\n",
    "def decision_function(z, w, b):\n",
    "    return np.dot(w, z) + b\n",
    "\n",
    "# Regra de predição: y = 1 se f(z) > 0, caso contrário 0\n",
    "def predict(z, w, b):\n",
    "    f_z = decision_function(z, w, b)\n",
    "    y = 1 if f_z > 0 else 0\n",
    "    return y, f_z\n",
    "\n",
    "# Cálculo do vetor peso\n",
    "w = calculate_w(support_vectors, dual_coefs)\n",
    "print(f\"Vetor peso w: {w}\")\n",
    "\n",
    "# Vetor de entrada para teste\n",
    "z = np.array([0, 0])\n",
    "\n",
    "# Calcula a saída da função de decisão e a predição\n",
    "y_pred, f_z = predict(z, w, bias)\n",
    "print(f\"f(z) para z = {z}: {f_z}\")\n",
    "print(f\"Predição para z = {z}: {y_pred}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b95ea3d-7867-43a0-a707-a2f05dc22e99",
   "metadata": {},
   "source": [
    "## Questão 5 – Interpretação do resultado do projeto de SVMs\n",
    "\n",
    "Foi realizado o treinamento de quatro SVMs usando diferentes kernels:\n",
    "\n",
    "- **SVM 1:** LinearSVC (kernel linear)\n",
    "- **SVM 2:** SVC com kernel linear\n",
    "- **SVM 3:** SVC com kernel RBF (Gaussiano)\n",
    "- **SVM 4:** SVC com kernel polinomial (grau 3)\n",
    "\n",
    "### a) Expressões das funções de decisão\n",
    "\n",
    "Para a SVM 2 (linear), a função de decisão é dada por:\n",
    "\n",
    "\\[\n",
    "f(z) = \\sum_{i=1}^3 \\lambda_i \\, \\langle \\mathbf{x}_i, z \\rangle + b\n",
    "\\]\n",
    "\n",
    "onde\n",
    "\n",
    "- Vetores de suporte \\(\\mathbf{x}_i\\):\n",
    "\n",
    "\\[\n",
    "[0, -4], \\quad [-1, 2], \\quad [-2, -2]\n",
    "\\]\n",
    "\n",
    "- Coeficientes duais \\(\\lambda_i\\):\n",
    "\n",
    "\\[\n",
    "[-0.4599, -0.2799, 0.7399]\n",
    "\\]\n",
    "\n",
    "- Bias:\n",
    "\n",
    "\\[\n",
    "b = -1.7995\n",
    "\\]\n",
    "\n",
    "Para as SVMs 3 (RBF) e 4 (polinomial), a função de decisão é:\n",
    "\n",
    "\\[\n",
    "f(z) = \\sum_i \\lambda_i \\, K(z, \\mathbf{x}_i) + b\n",
    "\\]\n",
    "\n",
    "onde \\(K(\\cdot, \\cdot)\\) é o kernel (RBF ou polinomial) e os \\(\\lambda_i\\), vetores de suporte e bias são dados no treinamento.\n",
    "\n",
    "---\n",
    "\n",
    "### b) Forma perceptron das SVMs 1 e 2 (lineares)\n",
    "\n",
    "A função de decisão pode ser escrita como:\n",
    "\n",
    "\\[\n",
    "f(z) = \\langle w, z \\rangle + b\n",
    "\\]\n",
    "\n",
    "com vetor peso\n",
    "\n",
    "\\[\n",
    "w = \\sum_i \\lambda_i \\mathbf{x}_i\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### c) Passos para converter SVM 2 para perceptron e economia computacional\n",
    "\n",
    "1. Calcular o vetor peso \\(w\\) como a soma ponderada dos vetores de suporte.\n",
    "2. A função de decisão passa a usar apenas um produto interno entre \\(w\\) e o vetor de entrada \\(z\\), somado ao bias \\(b\\).\n",
    "3. A economia computacional é grande porque se reduz o número de produtos internos de 3 (vetores de suporte) para 1.\n",
    "\n",
    "---\n",
    "\n",
    "### d) Informações das SVMs 3 e 4 (não-lineares)\n",
    "\n",
    "- SVM 3 (RBF):\n",
    "  - Número total de vetores de suporte: 6\n",
    "  - Índices dos vetores de suporte e coeficientes duals conforme saída do modelo\n",
    "  - Bias: aproximadamente \\(-0.0867\\)\n",
    "\n",
    "- SVM 4 (polinomial):\n",
    "  - Número total de vetores de suporte: 3\n",
    "  - Índices e coeficientes conforme saída\n",
    "  - Bias: aproximadamente \\(-1.0373\\)\n",
    "\n",
    "---\n",
    "\n",
    "### e) Exemplo menos “confiante” e predição\n",
    "\n",
    "- A menor magnitude (valor próximo a zero) entre as saídas da função de decisão no conjunto treino indica a menor confiança.\n",
    "- No exemplo dado, o valor é aproximadamente \\(0.9099\\), com predição para a classe positiva (1).\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9550d097-bd8f-4141-8d8b-46a7d21785f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vetor peso w: [-1.19980506 -0.19980504]\n",
      "f(z) para z = [1 1]: -3.19915523\n",
      "Predição para z = [1 1]: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Vetores de suporte da SVM 2\n",
    "support_vectors = np.array([\n",
    "    [0, -4],\n",
    "    [-1, 2],\n",
    "    [-2, -2]\n",
    "])\n",
    "\n",
    "# Coeficientes duais\n",
    "dual_coefs = np.array([-0.45994152, -0.27992202, 0.73986354])\n",
    "\n",
    "# Bias\n",
    "bias = -1.79954513\n",
    "\n",
    "# Calcula vetor peso w\n",
    "w = np.sum(dual_coefs[:, np.newaxis] * support_vectors, axis=0)\n",
    "print(f\"Vetor peso w: {w}\")\n",
    "\n",
    "# Vetor de entrada exemplo (pode alterar para testar)\n",
    "z = np.array([1, 1])\n",
    "\n",
    "# Função de decisão f(z)\n",
    "f_z = np.dot(w, z) + bias\n",
    "print(f\"f(z) para z = {z}: {f_z}\")\n",
    "\n",
    "# Predição\n",
    "y_pred = 1 if f_z > 0 else 0\n",
    "print(f\"Predição para z = {z}: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e51b69a-8bff-46b3-afd8-9c7383371daa",
   "metadata": {},
   "source": [
    "## Questão 6 – Treinamento e avaliação de uma SVM linear com C=1\n",
    "\n",
    "Nesta questão, devemos:\n",
    "\n",
    "1. Treinar uma SVM com kernel linear e hiperparâmetro \\( C = 1 \\) usando o conjunto de treino `dataset_train.txt`.\n",
    "2. Avaliar o desempenho do modelo no conjunto de teste `dataset_test.txt`.\n",
    "3. Converter a SVM treinada para um perceptron, estimando o custo computacional durante a fase de teste e avaliando a economia em termos de memória e número de operações.\n",
    "\n",
    "---\n",
    "\n",
    "### Passos gerais para a solução\n",
    "\n",
    "- **Pré-processamento:** Normalizar os dados de treino para média zero e variância um, aplicando os mesmos fatores de normalização ao conjunto de teste.\n",
    "- **Treinamento:** Usar `sklearn.svm.SVC` com kernel linear e `C=1`.\n",
    "- **Avaliação:** Calcular a acurácia ou outro métrico no conjunto de teste.\n",
    "- **Conversão para perceptron:** Extrair vetor peso \\( w \\) e bias \\( b \\) da SVM para implementar a decisão como:\n",
    "\n",
    "\\[\n",
    "f(z) = \\langle w, z \\rangle + b\n",
    "\\]\n",
    "\n",
    "- **Análise de custo:** Comparar o custo computacional do modelo original (baseado nos vetores de suporte) com o perceptron.\n",
    "\n",
    "---\n",
    "\n",
    "### Código de exemplo usando scikit-learn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9689208-36bb-44ca-ae28-30deb134ecd7",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "dataset_train.txt not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Carregar dados (substitua pelos seus arquivos reais)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Assumindo que os arquivos tenham as features nas colunas e a última coluna o rótulo\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m train_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mloadtxt(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset_train.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m test_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mloadtxt(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset_test.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m train_data[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], train_data[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\lib\\npyio.py:1373\u001b[0m, in \u001b[0;36mloadtxt\u001b[1;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, quotechar, like)\u001b[0m\n\u001b[0;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(delimiter, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m   1371\u001b[0m     delimiter \u001b[38;5;241m=\u001b[39m delimiter\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 1373\u001b[0m arr \u001b[38;5;241m=\u001b[39m _read(fname, dtype\u001b[38;5;241m=\u001b[39mdtype, comment\u001b[38;5;241m=\u001b[39mcomment, delimiter\u001b[38;5;241m=\u001b[39mdelimiter,\n\u001b[0;32m   1374\u001b[0m             converters\u001b[38;5;241m=\u001b[39mconverters, skiplines\u001b[38;5;241m=\u001b[39mskiprows, usecols\u001b[38;5;241m=\u001b[39musecols,\n\u001b[0;32m   1375\u001b[0m             unpack\u001b[38;5;241m=\u001b[39munpack, ndmin\u001b[38;5;241m=\u001b[39mndmin, encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   1376\u001b[0m             max_rows\u001b[38;5;241m=\u001b[39mmax_rows, quote\u001b[38;5;241m=\u001b[39mquotechar)\n\u001b[0;32m   1378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\lib\\npyio.py:992\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(fname, delimiter, comment, quote, imaginary_unit, usecols, skiplines, max_rows, converters, ndmin, unpack, dtype, encoding)\u001b[0m\n\u001b[0;32m    990\u001b[0m     fname \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(fname)\n\u001b[0;32m    991\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fname, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 992\u001b[0m     fh \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlib\u001b[38;5;241m.\u001b[39m_datasource\u001b[38;5;241m.\u001b[39mopen(fname, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrt\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39mencoding)\n\u001b[0;32m    993\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m encoding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    994\u001b[0m         encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(fh, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\lib\\_datasource.py:193\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03mOpen `path` with `mode` and return the file object.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    189\u001b[0m \n\u001b[0;32m    190\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    192\u001b[0m ds \u001b[38;5;241m=\u001b[39m DataSource(destpath)\n\u001b[1;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\u001b[38;5;241m.\u001b[39mopen(path, mode, encoding\u001b[38;5;241m=\u001b[39mencoding, newline\u001b[38;5;241m=\u001b[39mnewline)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\lib\\_datasource.py:533\u001b[0m, in \u001b[0;36mDataSource.open\u001b[1;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _file_openers[ext](found, mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m    531\u001b[0m                               encoding\u001b[38;5;241m=\u001b[39mencoding, newline\u001b[38;5;241m=\u001b[39mnewline)\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 533\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: dataset_train.txt not found."
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Carregar dados (substitua pelos seus arquivos reais)\n",
    "# Assumindo que os arquivos tenham as features nas colunas e a última coluna o rótulo\n",
    "train_data = np.loadtxt('dataset_train.txt')\n",
    "test_data = np.loadtxt('dataset_test.txt')\n",
    "\n",
    "X_train, y_train = train_data[:, :-1], train_data[:, -1]\n",
    "X_test, y_test = test_data[:, :-1], test_data[:, -1]\n",
    "\n",
    "# Normalização com base no conjunto de treino\n",
    "scaler = StandardScaler()\n",
    "X_train_norm = scaler.fit_transform(X_train)\n",
    "X_test_norm = scaler.transform(X_test)\n",
    "\n",
    "# Treinamento da SVM linear com C=1\n",
    "svm = SVC(kernel='linear', C=1)\n",
    "svm.fit(X_train_norm, y_train)\n",
    "\n",
    "# Avaliação no conjunto de teste\n",
    "accuracy = svm.score(X_test_norm, y_test)\n",
    "print(f\"Acurácia no conjunto de teste: {accuracy:.4f}\")\n",
    "\n",
    "# Conversão para perceptron: extrair vetor peso e bias\n",
    "w = svm.coef_[0]\n",
    "b = svm.intercept_[0]\n",
    "print(f\"Vetor peso w: {w}\")\n",
    "print(f\"Bias b: {b}\")\n",
    "\n",
    "# Função de decisão como perceptron para os dois primeiros exemplos do teste\n",
    "for i in range(2):\n",
    "    z = X_test_norm[i]\n",
    "    f_z = np.dot(w, z) + b\n",
    "    y_pred = 1 if f_z > 0 else 0\n",
    "    print(f\"Exemplo {i+1}: f(z)={f_z:.4f}, predição={y_pred}, label verdadeiro={int(y_test[i])}\")\n",
    "\n",
    "# Estimativa do custo computacional\n",
    "num_SVs = len(svm.support_)\n",
    "K = X_train.shape[1]\n",
    "cost_svm = num_SVs * (K + (K - 1))\n",
    "cost_perceptron = K + (K - 1)\n",
    "reduction_factor = cost_svm / cost_perceptron\n",
    "\n",
    "print(f\"Número de vetores de suporte: {num_SVs}\")\n",
    "print(f\"Custo computacional SVM: {cost_svm} operações\")\n",
    "print(f\"Custo computacional perceptron: {cost_perceptron} operações\")\n",
    "print(f\"Fator de redução do custo: {reduction_factor:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d13cf3-cfa1-450f-854b-4843ec45a13d",
   "metadata": {},
   "source": [
    "## Questão 7 – Otimização de hiperparâmetros da SVM com kernel Gaussiano (RBF)\n",
    "\n",
    "### a) Treinamento com diferentes combinações de \\( C \\) e \\( \\gamma \\)\n",
    "\n",
    "Os hiperparâmetros a serem avaliados são:\n",
    "\n",
    "- \\( C = \\{0.01, 1, 100\\} \\)\n",
    "- \\( \\gamma = \\{0.5, 1\\} \\)\n",
    "\n",
    "O objetivo é treinar 6 SVMs distintas, cada uma com uma combinação desses valores, utilizando o conjunto de treino, e avaliar seu desempenho no conjunto de validação para selecionar a melhor configuração.\n",
    "\n",
    "---\n",
    "\n",
    "### b) Parâmetros do classificador escolhido\n",
    "\n",
    "Para o melhor modelo, deve-se indicar:\n",
    "\n",
    "- \\( \\gamma \\) escolhido (parâmetro do kernel RBF)\n",
    "- Número de vetores de suporte (SVs)\n",
    "- Índices dos SVs no conjunto de treino\n",
    "- Valores dos coeficientes duais (\\(\\lambda_i\\))\n",
    "- Valor do bias/intercepto \\( b \\)\n",
    "\n",
    "---\n",
    "\n",
    "### c) Comparação numérica da função de decisão\n",
    "\n",
    "Comparar o resultado da função de decisão \\( f(z) = \\sum_i \\lambda_i K(z, x_i) + b \\) calculada manualmente com a saída da função `decision_function()` do modelo scikit-learn para os dois primeiros exemplos do conjunto de teste.\n",
    "\n",
    "---\n",
    "\n",
    "## Exemplo de código para realizar essa tarefa\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b139accd-1c81-4839-bace-a8e0342a33ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Carregar dados (substitua pelos seus arquivos reais)\n",
    "train_data = np.loadtxt('dataset_train.txt')\n",
    "val_data = np.loadtxt('dataset_validation.txt')\n",
    "test_data = np.loadtxt('dataset_test.txt')\n",
    "\n",
    "X_train, y_train = train_data[:, :-1], train_data[:, -1]\n",
    "X_val, y_val = val_data[:, :-1], val_data[:, -1]\n",
    "X_test, y_test = test_data[:, :-1], test_data[:, -1]\n",
    "\n",
    "# Normalização\n",
    "scaler = StandardScaler()\n",
    "X_train_norm = scaler.fit_transform(X_train)\n",
    "X_val_norm = scaler.transform(X_val)\n",
    "X_test_norm = scaler.transform(X_test)\n",
    "\n",
    "# Hiperparâmetros a testar\n",
    "C_values = [0.01, 1, 100]\n",
    "gamma_values = [0.5, 1]\n",
    "\n",
    "best_accuracy = 0\n",
    "best_params = {}\n",
    "best_model = None\n",
    "\n",
    "for C in C_values:\n",
    "    for gamma in gamma_values:\n",
    "        svm = SVC(kernel='rbf', C=C, gamma=gamma)\n",
    "        svm.fit(X_train_norm, y_train)\n",
    "        y_val_pred = svm.predict(X_val_norm)\n",
    "        acc = accuracy_score(y_val, y_val_pred)\n",
    "        print(f\"C={C}, gamma={gamma}, Val Accuracy={acc:.4f}\")\n",
    "        if acc > best_accuracy:\n",
    "            best_accuracy = acc\n",
    "            best_params = {'C': C, 'gamma': gamma}\n",
    "            best_model = svm\n",
    "\n",
    "print(\"\\nMelhor modelo:\")\n",
    "print(best_params)\n",
    "print(f\"Acurácia no conjunto de validação: {best_accuracy:.4f}\")\n",
    "\n",
    "# Informações do melhor modelo\n",
    "print(f\"Número de vetores de suporte: {len(best_model.support_)}\")\n",
    "print(f\"Índices dos vetores de suporte: {best_model.support_}\")\n",
    "print(f\"Coeficientes duais: {best_model.dual_coef_}\")\n",
    "print(f\"Bias (intercepto): {best_model.intercept_}\")\n",
    "\n",
    "# Comparar função de decisão manual e scikit-learn para os dois primeiros exemplos do teste\n",
    "for i in range(2):\n",
    "    z = X_test_norm[i]\n",
    "    # Cálculo manual da função de decisão\n",
    "    sv = best_model.support_vectors_\n",
    "    dual_coefs = best_model.dual_coef_[0]\n",
    "    gamma = best_model._gamma  # valor de gamma utilizado\n",
    "    # Kernel RBF\n",
    "    kernel_vals = np.exp(-gamma * np.linalg.norm(sv - z, axis=1)**2)\n",
    "    f_manual = np.sum(dual_coefs * kernel_vals) + best_model.intercept_[0]\n",
    "    \n",
    "    # Função de decisão do sklearn\n",
    "    f_sklearn = best_model.decision_function(z.reshape(1, -1))[0]\n",
    "    \n",
    "    print(f\"\\nExemplo {i+1}:\")\n",
    "    print(f\"f_manual = {f_manual:.6f}\")\n",
    "    print(f\"f_sklearn = {f_sklearn:.6f}\")\n",
    "    print(f\"Diferença = {abs(f_manual - f_sklearn):.8f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
